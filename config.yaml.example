# Jafar - Application Configuration
# Copy this file to config.yaml and customize as needed.
# Secrets (API keys, passwords) should go in .env file, not here.

# Application Settings
app:
  # spaCy NLP model for entity extraction
  # The default model (en_core_web_lg) is bundled with the package.
  # To use a different model, install it and change this value:
  #   - en_core_web_sm: Small, fast, less accurate
  #   - en_core_web_lg: Large, more accurate (default, bundled)
  #   - en_core_web_trf: Transformer-based, most accurate (slowest, requires GPU)
  spacy_model: en_core_web_lg

# Twitter/twscrape Settings
twitter:
  # Path to twscrape SQLite database
  db_path: accounts.db

  # SOCKS5 proxies (optional)
  # These proxies are assigned to accounts when using add_account.py
  # Each account gets a proxy in round-robin order for consistent IPs.
  # Leave empty here after accounts are added - proxies are stored in accounts.db
  proxies:
    # - socks5://user:pass@host1:port
    # - socks5://user:pass@host2:port

# LLM Provider Settings
llm:
  # Provider: "openai" or "google"
  provider: google

  # Model names (API keys go in .env)
  # OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, etc.
  # Google: gemini-2.0-flash, gemini-1.5-pro, gemini-1.5-flash, etc.
  openai_model: gpt-4o
  google_model: gemini-2.0-flash

# Email/SMTP Settings
smtp:
  host: smtp.gmail.com
  port: 587
  use_tls: true
  # Username and password go in .env

email:
  from: your_email@gmail.com
  from_name: Jafar Intelligence System
  to:
    - recipient@example.com
  # Multiple recipients:
  # to:
  #   - recipient1@example.com
  #   - recipient2@example.com

  # Admin diagnostics email settings
  admin:
    # Enable admin diagnostics emails (always sent on errors, optional for successful runs)
    enabled: true

    # Send admin email even on successful runs (for monitoring)
    send_on_success: false

    # Admin email recipients (defaults to main recipients if not specified)
    recipients:
      - admin@example.com
    # Leave empty to use main recipients:
    # recipients: []

    # Keep N log files (older logs will be deleted automatically)
    log_retention_count: 10

# Scraping Limits
scraping:
  # Tweets per broad topic during discovery phase
  # Each worker paces at ~10-15s per page (15 tweets), so:
  #   50 tweets  ≈ 30-45s per topic
  #   100 tweets ≈ 60-90s per topic
  #   200 tweets ≈ 2-3 min per topic
  # More accounts = more parallel workers (faster overall), NOT higher limits
  # Recommended: 50-100 for balanced speed/coverage
  broad_tweet_limit: 75

  # Tweets per discovered trend (deep dive phase)
  # Usually fewer needed since you're drilling into specific entities
  # Recommended: 30-75
  specific_tweet_limit: 50

  # Number of top trends to analyze in depth
  # More trends = longer deep dive phase
  top_trends_count: 15

  # Minimum thresholds for trend detection
  # Lower = more trends (including noise), Higher = only strong signals
  min_trend_mentions: 3
  min_trend_authors: 2

  # Timeout for each search query (seconds)
  # Prevents hanging if Twitter API becomes unresponsive
  search_timeout: 120

# Broad Search Topics
# These topics cast a wide net to discover trending financial content.
# The analyzer then extracts specific entities from the results.
# Use OR for alternatives within a topic.
broad_topics:
  # Financial Twitter (traditional signals)
  - fintwit
  - markets

  # PRICE SIGNALS (direct inflation indicators)
  - price increase OR price hike
  - too expensive OR can't afford
  - shrinkflation OR smaller portions

  # SUPPLY/AVAILABILITY (shortages = pricing power)
  - shortage OR sold out
  - wait list OR backorder
  - supply OR allocation
  - in stock OR restocked

  # SUPPLY CHAIN (disruption signals)
  - supply chain OR shipping delays
  - chip shortage OR semiconductor
  - port congestion OR freight

  # BREAKING ECONOMIC NEWS (general discovery)
  - breaking
  - just announced
  - developing

  # CONSUMER SPENDING BEHAVIOR (recession signals)
  - consumer spending OR retail sales
  - cutting back OR tightening budget
  - splurging OR treating myself
  - worth it OR not worth it

  # CREDIT/DEBT (recession leading indicator)
  - credit card debt OR maxed out
  - can't pay bills OR late payment
  - bankruptcy OR foreclosure

  # EMPLOYMENT/WAGES (income side of economy)
  - hiring OR layoffs
  - pay cut OR pay raise
  - wage OR salary

  # SMALL BUSINESS HEALTH (leading indicator)
  - small business closing OR going out of business
  - store closing OR shutting down

  # COST OF LIVING (broad inflation signals)
  - inflation OR cost of living
  - groceries OR grocery prices
  - gas prices OR energy costs
  - rent OR housing costs

  # INSURANCE/HEALTHCARE (hidden inflation)
  - insurance rates OR premium increase
  - medical bills OR healthcare costs

  # DISCRETIONARY SPENDING (consumer confidence)
  - canceling subscription OR cutting streaming
  - flight prices OR airfare
  - vacation cost OR travel expensive

  # TARIFFS/TRADE (policy-driven price changes)
  - tariff OR tariffs
  - import prices

  # PRODUCT LAUNCHES (reveals pricing trends, demand)
  - launched OR released
  - available now OR pre-order
  - announced

  # SENTIMENT (economic confidence)
  - worried OR concerned
  - bullish OR bearish
  - panic OR euphoria

# News Fetching Settings (economic news roundup)
news:
  # Enable/disable news fetching for the daily digest
  enabled: true

  # Search queries for fetching economic news via DuckDuckGo
  queries:
    - "economy news today"
    - "stock market news"
    - "inflation consumer prices"
    - "Federal Reserve monetary policy"
    - "corporate earnings news"
    - "supply chain logistics"

  # Maximum results per query (more = broader coverage, slower)
  max_results_per_query: 5

# Vector Memory Settings (for historical parallels)
memory:
  # Enable/disable semantic memory system
  enabled: true

  # Storage backend: "chroma" (local, easy) or "pgvector" (production)
  store_type: chroma

  # Embedding provider: "openai" or "local" (free, requires sentence-transformers)
  embedding_provider: openai

  # OpenAI embedding model (only used when embedding_provider is "openai")
  # Options:
  #   - text-embedding-3-small: 1536 dimensions, faster, cheaper
  #   - text-embedding-3-large: 3072 dimensions, better quality
  openai_embedding_model: text-embedding-3-large

  # Override embedding dimensions (optional)
  # REQUIRED for pgvector which has a 2000 dimension limit (ivfflat index)!
  # When using text-embedding-3-large with pgvector, set this to 2000 for max quality.
  # The model supports native dimension reduction while retaining quality.
  # Leave commented out for ChromaDB (no limit) or text-embedding-3-small.
  # embedding_dimensions: 2000

  # ChromaDB storage path (for local chroma store)
  chroma_path: ./memory_store

  # Minimum similarity for finding historical parallels (0.0-1.0)
  # Higher = stricter matching, fewer but more relevant parallels
  min_similarity: 0.6

# Fact Checker Settings (market data verification)
fact_checker:
  # Enable/disable real-time market data fact-checking
  enabled: true

  # Cache market data for N minutes to avoid excessive API calls
  cache_ttl_minutes: 5

  # Allowed variance for "price at X" claims (percentage)
  price_tolerance_pct: 2.0

# Temporal Trend Tracking (multi-day trend analysis)
temporal:
  # Number of consecutive days to flag as "developing story"
  # Example: Set to 3 to highlight trends appearing 3+ days in a row
  # Recommended: 3 (balances noise filtering with early detection)
  consecutive_threshold: 3

  # Gap period (in days) to consider a "new episode" vs continuation
  # Example: Set to 14 so a trend appearing after 14+ days of silence is flagged as "recurring"
  # Recommended: 14 (two-week gaps indicate distinct events)
  gap_threshold_days: 14

# Logging
logging:
  # Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: INFO