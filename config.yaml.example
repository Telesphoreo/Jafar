# Twitter Sentiment Analysis - Application Configuration
# Copy this file to config.yaml and customize as needed.
# Secrets (API keys, passwords) should go in .env file, not here.

# Application Settings
app:
  # spaCy NLP model for entity extraction
  # Options:
  #   - en_core_web_sm: Small, fast, less accurate (default)
  #   - en_core_web_lg: Large, more accurate
  #   - en_core_web_trf: Transformer-based, most accurate (slowest, requires GPU recommended)
  # IMPORTANT: You must download the model first: python -m spacy download <model_name>
  spacy_model: en_core_web_sm

# Twitter/twscrape Settings
twitter:
  # Path to twscrape SQLite database
  db_path: accounts.db

  # SOCKS5 proxies (optional)
  # These proxies are assigned to accounts when using add_account.py
  # Each account gets a proxy in round-robin order for consistent IPs.
  # Leave empty here after accounts are added - proxies are stored in accounts.db
  proxies:
    # - socks5://user:pass@host1:port
    # - socks5://user:pass@host2:port

# LLM Provider Settings
llm:
  # Provider: "openai" or "google"
  provider: openai

  # Model names (API keys go in .env)
  openai_model: gpt-4o
  google_model: gemini-2.0-flash

# Email/SMTP Settings
smtp:
  host: smtp.gmail.com
  port: 587
  use_tls: true
  # Username and password go in .env

email:
  from: your_email@gmail.com
  to:
    - recipient@example.com
  # Multiple recipients:
  # to:
  #   - recipient1@example.com
  #   - recipient2@example.com

  # Admin diagnostics email settings
  admin:
    # Enable admin diagnostics emails (always sent on errors, optional for successful runs)
    enabled: true

    # Send admin email even on successful runs (for monitoring)
    send_on_success: false

    # Admin email recipients (defaults to main recipients if not specified)
    recipients:
      - admin@example.com
    # Leave empty to use main recipients:
    # recipients: []

    # Keep N log files (older logs will be deleted automatically)
    log_retention_count: 10

# Scraping Limits
scraping:
  # Tweets per broad topic during discovery phase
  # Each worker paces at ~10-15s per page (15 tweets), so:
  #   50 tweets  ≈ 30-45s per topic
  #   100 tweets ≈ 60-90s per topic
  #   200 tweets ≈ 2-3 min per topic
  # More accounts = more parallel workers (faster overall), NOT higher limits
  # Recommended: 50-100 for balanced speed/coverage
  broad_tweet_limit: 75

  # Tweets per discovered trend (deep dive phase)
  # Usually fewer needed since you're drilling into specific entities
  # Recommended: 30-75
  specific_tweet_limit: 50

  # Number of top trends to analyze in depth
  # More trends = longer deep dive phase
  top_trends_count: 15

  # Minimum thresholds for trend detection
  # Lower = more trends (including noise), Higher = only strong signals
  min_trend_mentions: 3
  min_trend_authors: 2

  # Timeout for each search query (seconds)
  # Prevents hanging if Twitter API becomes unresponsive
  search_timeout: 120

# Broad Search Topics
# These topics cast a wide net to discover trending financial content.
# The analyzer then extracts specific entities from the results.
broad_topics:
  # General financial Twitter communities
  - fintwit
  - stock market today
  - trading
  - markets

  # Breaking/emerging signals
  - breaking market
  - just announced
  - shortage OR surplus
  - supply chain

  # Commodities
  - commodities
  - futures
  - spot price

  # Options flow - often leads price
  - unusual volume
  - options flow
  - dark pool
  - whale alert

  # Sector rotation signals
  - sector rotation
  - money flowing
  - outperform OR underperform

  # Macro signals
  - inflation data
  - yield curve
  - dollar index

  # International markets
  - asia markets
  - europe open
  - emerging markets

  # Sentiment extremes
  - oversold OR overbought
  - capitulation
  - FOMO OR panic

  # Earnings/events
  - earnings surprise
  - guidance raised OR lowered
  - FDA approval OR rejection

  # Physical markets
  - physical delivery
  - warehouse inventory
  - shipping rates
  - freight

# Vector Memory Settings (for historical parallels)
memory:
  # Enable/disable semantic memory system
  enabled: true

  # Storage backend: "chroma" (local, easy) or "pgvector" (production)
  store_type: chroma

  # Embedding provider: "openai" or "local" (free, requires sentence-transformers)
  embedding_provider: openai

  # OpenAI embedding model (only used when embedding_provider is "openai")
  # Options:
  #   - text-embedding-3-small: 1536 dimensions, faster, cheaper
  #   - text-embedding-3-large: 3072 dimensions, better quality
  openai_embedding_model: text-embedding-3-large

  # Override embedding dimensions (optional)
  # REQUIRED for pgvector which has a 2000 dimension limit (ivfflat index)!
  # When using text-embedding-3-large with pgvector, set this to 2000 for max quality.
  # The model supports native dimension reduction while retaining quality.
  # Leave commented out for ChromaDB (no limit) or text-embedding-3-small.
  # embedding_dimensions: 2000

  # ChromaDB storage path (for local chroma store)
  chroma_path: ./memory_store

  # Minimum similarity for finding historical parallels (0.0-1.0)
  # Higher = stricter matching, fewer but more relevant parallels
  min_similarity: 0.6

# Fact Checker Settings (market data verification)
fact_checker:
  # Enable/disable real-time market data fact-checking
  enabled: true

  # Cache market data for N minutes to avoid excessive API calls
  cache_ttl_minutes: 5

  # Allowed variance for "price at X" claims (percentage)
  price_tolerance_pct: 2.0

# Temporal Trend Tracking (multi-day trend analysis)
temporal:
  # Number of consecutive days to flag as "developing story"
  # Example: Set to 3 to highlight trends appearing 3+ days in a row
  # Recommended: 3 (balances noise filtering with early detection)
  consecutive_threshold: 3

  # Gap period (in days) to consider a "new episode" vs continuation
  # Example: Set to 14 so a trend appearing after 14+ days of silence is flagged as "recurring"
  # Recommended: 14 (two-week gaps indicate distinct events)
  gap_threshold_days: 14

# Logging
logging:
  # Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: INFO