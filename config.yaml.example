# =============================================================================
# Twitter Sentiment Analysis - Application Configuration
# =============================================================================
# Copy this file to config.yaml and customize as needed.
# Secrets (API keys, passwords) should go in .env file, not here.

# -----------------------------------------------------------------------------
# Twitter/twscrape Settings
# -----------------------------------------------------------------------------
twitter:
  # Path to twscrape SQLite database
  db_path: accounts.db

  # SOCKS5 proxies bound to accounts (optional)
  # Each account gets assigned a proxy in round-robin fashion.
  # This keeps each account on a consistent IP to avoid detection.
  # Credentials should be in .env as TWITTER_PROXIES if they contain passwords.
  proxies: []
  # Example with multiple proxies:
  # proxies:
  #   - socks5://host1:port
  #   - socks5://host2:port
  # With 3 accounts and 2 proxies:
  #   account1 -> proxy1, account2 -> proxy2, account3 -> proxy1

# -----------------------------------------------------------------------------
# LLM Provider Settings
# -----------------------------------------------------------------------------
llm:
  # Provider: "openai" or "google"
  provider: openai

  # Model names (API keys go in .env)
  openai_model: gpt-4o
  google_model: gemini-2.0-flash

# -----------------------------------------------------------------------------
# Email/SMTP Settings
# -----------------------------------------------------------------------------
smtp:
  host: smtp.gmail.com
  port: 587
  use_tls: true
  # Username and password go in .env

email:
  from: your_email@gmail.com
  to:
    - recipient@example.com
  # Multiple recipients:
  # to:
  #   - recipient1@example.com
  #   - recipient2@example.com

# -----------------------------------------------------------------------------
# Scraping Limits
# -----------------------------------------------------------------------------
scraping:
  # Tweets per broad topic (higher = better discovery but slower)
  # With 1 account: 100-200 is reasonable
  # With 5+ accounts: 500-1000 for comprehensive coverage
  broad_tweet_limit: 200

  # Tweets per discovered trend
  specific_tweet_limit: 100

  # Number of top trends to analyze in depth
  top_trends_count: 10

  # Minimum thresholds for trend detection
  # Lower = more trends (including noise), Higher = only strong signals
  min_trend_mentions: 3
  min_trend_authors: 2

# -----------------------------------------------------------------------------
# Broad Search Topics
# -----------------------------------------------------------------------------
# These topics cast a wide net to discover trending financial content.
# The analyzer then extracts specific entities from the results.
broad_topics:
  # General financial Twitter communities
  - fintwit
  - stock market today
  - trading
  - markets

  # Breaking/emerging signals
  - breaking market
  - just announced
  - shortage OR surplus
  - supply chain

  # Commodities
  - commodities
  - futures
  - spot price

  # Options flow - often leads price
  - unusual volume
  - options flow
  - dark pool
  - whale alert

  # Sector rotation signals
  - sector rotation
  - money flowing
  - outperform OR underperform

  # Macro signals
  - inflation data
  - yield curve
  - dollar index

  # International markets
  - asia markets
  - europe open
  - emerging markets

  # Sentiment extremes
  - oversold OR overbought
  - capitulation
  - FOMO OR panic

  # Earnings/events
  - earnings surprise
  - guidance raised OR lowered
  - FDA approval OR rejection

  # Physical markets
  - physical delivery
  - warehouse inventory
  - shipping rates
  - freight

# -----------------------------------------------------------------------------
# Vector Memory Settings (for historical parallels)
# -----------------------------------------------------------------------------
memory:
  # Enable/disable semantic memory system
  enabled: true

  # Storage backend: "chroma" (local, easy) or "pgvector" (production)
  store_type: chroma

  # Embedding provider: "openai" or "local" (free, requires sentence-transformers)
  embedding_provider: openai

  # ChromaDB storage path (for local chroma store)
  chroma_path: ./memory_store

  # Minimum similarity for finding historical parallels (0.0-1.0)
  # Higher = stricter matching, fewer but more relevant parallels
  min_similarity: 0.6

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
logging:
  # Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: INFO
